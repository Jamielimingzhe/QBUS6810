{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5741ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a478d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori = pd.read_csv('train.csv')\n",
    "test_ori = pd.read_csv('test.csv')\n",
    "y_ori = train_ori['price']\n",
    "y = np.log(y_ori) #4000 entries\n",
    "\n",
    "#combine train and test data to do feature engineering for predictors\n",
    "train = pd.concat([train_ori.drop('price',axis=1),test_ori],axis=0,ignore_index=True) #6000 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb3c57",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f35768",
   "metadata": {},
   "source": [
    "## Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dde9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 34574.85it/s]\n",
      "  7%|▋         | 444/6000 [00:07<01:16, 72.71it/s]"
     ]
    }
   ],
   "source": [
    "train['amenities_clean'] = np.nan\n",
    "train['amenities_clean'] = train['amenities_clean'].astype('object')\n",
    "\n",
    "for i in tqdm(range(len(train['amenities']))):\n",
    "    train.at[i,'amenities_clean'] = train.loc[i,'amenities'][2:-2].split('\", \"')\n",
    "    \n",
    "for i in tqdm(range(len(train['amenities_clean']))):\n",
    "    for j in range(len(train.loc[i,'amenities_clean'])):\n",
    "        if 'Fast wifi \\\\u2013' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Fast Wifi'\n",
    "        if 'Wifi \\\\u2013' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Wifi'\n",
    "        if 'HDTV' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'HD Television'\n",
    "        if 'TV' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'TV'\n",
    "        if 'body soap' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Body soap'\n",
    "        if 'shampoo' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Shampoo'\n",
    "        if 'refrigerator' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Refrigerator'\n",
    "        if 'fridge' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Refrigerator'\n",
    "        if 'conditioner' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Conditioner'\n",
    "        if 'stove' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Stove'\n",
    "        if 'oven' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Oven'\n",
    "        if 'sound system' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Sound system'\n",
    "        if 'Sound system' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Sound system'\n",
    "        if 'Clothing storage' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Clothing storage'\n",
    "        if 'Children\\\\u2019s books and toys' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Children\\\\u2019s books and toys'\n",
    "        if 'Shared hot tub' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Hot tub'\n",
    "        if 'Private hot tub' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private hot tub'\n",
    "        if 'Shared pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Pool'\n",
    "        if 'Shared indoor pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Pool'\n",
    "        if 'Shared outdoor pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Pool'\n",
    "        if 'Private pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private pool'\n",
    "        if 'Private indoor pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private pool'\n",
    "        if 'Private outdoor pool' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private pool'\n",
    "        if 'Free washer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Washer'\n",
    "        if 'Paid washer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Paid Washer'\n",
    "        if 'Washer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Washer'\n",
    "        if 'Dryer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Dryer'\n",
    "        if 'Free dryer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Dryer'\n",
    "        if 'Paid dryer' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Paid Dryer'\n",
    "        if 'on premises' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Parking on premises'\n",
    "        if 'off premises' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Parking off premises'\n",
    "        if 'Free street parking' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Parking off premises'\n",
    "        if 'conditioning' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Air conditioning'\n",
    "        if 'Game console' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Game console'\n",
    "        if 'Gym' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Gym'\n",
    "        if 'gym' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Gym'\n",
    "        if 'coffee' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Coffee maker'\n",
    "        if 'sauna' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Sauna'\n",
    "        if 'high chair' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'High chair'\n",
    "        if 'High chair' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'High chair'\n",
    "        if 'crib' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Crib'\n",
    "        if 'Crib' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Crib'\n",
    "        if 'Fenced garden or backyard' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private garden or backyard'\n",
    "        if 'Private fenced garden or backyard' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private garden or backyard'\n",
    "        if 'Shared fenced garden or backyard' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Shared garden or backyard'\n",
    "        if 'Private fenced garden or backyard' in train.loc[i,'amenities_clean'][j]:\n",
    "            train.loc[i,'amenities_clean'][j] = 'Private garden or backyard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c905aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "amen_list = train['amenities_clean'].to_list()\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "bow = vectorizer.fit_transform(amen_list)\n",
    "bow_df = pd.DataFrame(bow.todense(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "train['description'][train['description'].isna()] = ''\n",
    "# train['neighborhood_overview'][train['neighborhood_overview'].isna()] = ''\n",
    "train['neighborhood_overview'] = train['neighborhood_overview'].fillna('NA')\n",
    "# corpus_des = [i for i in train['description']]\n",
    "corpus_des_neighbor = [i for i in train['neighborhood_overview']]\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# tfidf_vectorizer.fit(corpus_des)\n",
    "# tfidf = tfidf_vectorizer.transform(corpus_des)\n",
    "# features = pd.DataFrame(tfidf.todense(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# tfidf_vectorizer_neighbor = TfidfVectorizer(stop_words = 'english')\n",
    "# tfidf_vectorizer_neighbor.fit(corpus_des_neighbor)\n",
    "# tfidf_neighbor = tfidf_vectorizer_neighbor.transform(corpus_des)\n",
    "# features_neighbor = pd.DataFrame(tfidf_neighbor.todense(), columns = tfidf_vectorizer_neighbor.get_feature_names_out())\n",
    "\n",
    "# all_features = pd.concat([bow_df,features,features_neighbor],axis=1)\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# tfidf_vectorizer.fit(corpus_des)\n",
    "\n",
    "# tfidf = tfidf_vectorizer.transform(corpus_des)\n",
    "\n",
    "# features = pd.DataFrame(tfidf.todense(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "corpus_des = [i for i in train['description']]\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "count_vectorizer.fit(corpus_des)\n",
    "\n",
    "features = count_vectorizer.transform(corpus_des)\n",
    "\n",
    "beach_index = count_vectorizer.get_feature_names().index('beach')\n",
    "beach1 = np.array(features[:, beach_index].todense()).reshape(-1)\n",
    "beach = pd.DataFrame(beach1, columns=['beach_dummy'])\n",
    "\n",
    "apartment_index = count_vectorizer.get_feature_names().index('apartment')\n",
    "apartment1 = np.array(features[:, apartment_index].todense()).reshape(-1)\n",
    "apartment = pd.DataFrame(apartment1, columns=['apartment_dummy'])\n",
    "\n",
    "walk_index = count_vectorizer.get_feature_names().index('walk')\n",
    "walk1 = np.array(features[:, walk_index].todense()).reshape(-1)\n",
    "walk = pd.DataFrame(walk1, columns=['walk_dummy'])\n",
    "\n",
    "large_index = count_vectorizer.get_feature_names().index('large')\n",
    "large1 = np.array(features[:, large_index].todense()).reshape(-1)\n",
    "large = pd.DataFrame(large1, columns=['walk_dummy'])\n",
    "\n",
    "city_index = count_vectorizer.get_feature_names().index('city')\n",
    "city1 = np.array(features[:, city_index].todense()).reshape(-1)\n",
    "city = pd.DataFrame(city1, columns=['city_dummy'])\n",
    "\n",
    "views_index = count_vectorizer.get_feature_names().index('views')\n",
    "views1 = np.array(features[:, views_index].todense()).reshape(-1)\n",
    "views = pd.DataFrame(views1, columns=['views_dummy'])\n",
    "\n",
    "private_index = count_vectorizer.get_feature_names().index('private')\n",
    "private1 = np.array(features[:, private_index].todense()).reshape(-1)\n",
    "private = pd.DataFrame(private1, columns=['private_dummy'])\n",
    "\n",
    "home_index = count_vectorizer.get_feature_names().index('home')\n",
    "home1 = np.array(features[:, home_index].todense()).reshape(-1)\n",
    "home = pd.DataFrame(home1, columns=['home_dummy'])\n",
    "\n",
    "balcony_index = count_vectorizer.get_feature_names().index('balcony')\n",
    "balcony1 = np.array(features[:, balcony_index].todense()).reshape(-1)\n",
    "balcony = pd.DataFrame(balcony1, columns=['balcony_dummy'])\n",
    "\n",
    "bondi_index = count_vectorizer.get_feature_names().index('bondi')\n",
    "bondi1 = np.array(features[:, bondi_index].todense()).reshape(-1)\n",
    "bondi = pd.DataFrame(bondi1, columns=['bondi_dummy'])\n",
    "\n",
    "house_index = count_vectorizer.get_feature_names().index('house')\n",
    "house1 = np.array(features[:, house_index].todense()).reshape(-1)\n",
    "house = pd.DataFrame(house1, columns=['house_dummy'])\n",
    "\n",
    "corpus_des1 = [i for i in train['neighborhood_overview']]\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "count_vectorizer.fit(corpus_des1)\n",
    "\n",
    "feature = count_vectorizer.transform(corpus_des1)\n",
    "\n",
    "beachoverview_index = count_vectorizer.get_feature_names().index('beach')\n",
    "beach_overview = np.array(feature[:, beachoverview_index].todense()).reshape(-1)\n",
    "beach_overview = pd.DataFrame(beach_overview, columns=['beach_overview_dummy'])\n",
    "\n",
    "bondioverview_index = count_vectorizer.get_feature_names().index('bondi')\n",
    "bondi_overview = np.array(feature[:, bondioverview_index].todense()).reshape(-1)\n",
    "bondi_overview = pd.DataFrame(bondi_overview, columns=['bondi_overview_dummy'])\n",
    "\n",
    "walkoverview_index = count_vectorizer.get_feature_names().index('walk')\n",
    "walk_overview = np.array(feature[:, walkoverview_index].todense()).reshape(-1)\n",
    "walk_overview = pd.DataFrame(walk_overview, columns=['walk_overview_dummy'])\n",
    "\n",
    "cityoverview_index = count_vectorizer.get_feature_names().index('city')\n",
    "city_overview = np.array(feature[:, cityoverview_index].todense()).reshape(-1)\n",
    "city_overview = pd.DataFrame(city_overview, columns=['city_overview_dummy'])\n",
    "\n",
    "shortoverview_index = count_vectorizer.get_feature_names().index('short')\n",
    "short_overview = np.array(feature[:, shortoverview_index].todense()).reshape(-1)\n",
    "short_overview = pd.DataFrame(short_overview, columns=['short_overview_dummy'])\n",
    "\n",
    "centreoverview_index = count_vectorizer.get_feature_names().index('centre')\n",
    "centre_overview = np.array(feature[:, centreoverview_index].todense()).reshape(-1)\n",
    "centre_overview = pd.DataFrame(centre_overview, columns=['centre_overview_dummy'])\n",
    "\n",
    "habouroverview_index = count_vectorizer.get_feature_names().index('habour')\n",
    "habour_overview = np.array(feature[:, habouroverview_index].todense()).reshape(-1)\n",
    "habour_overview = pd.DataFrame(habour_overview, columns=['habour_overview_dummy'])\n",
    "\n",
    "walkingoverview_index = count_vectorizer.get_feature_names().index('walking')\n",
    "walking_overview = np.array(feature[:, walkingoverview_index].todense()).reshape(-1)\n",
    "walking_overview = pd.DataFrame(walking_overview, columns=['walk_overview_dummy'])\n",
    "\n",
    "cbdingoverview_index = count_vectorizer.get_feature_names().index('cbd')\n",
    "cbding_overview = np.array(feature[:, cbdingoverview_index].todense()).reshape(-1)\n",
    "cbding_overview = pd.DataFrame(cbding_overview, columns=['cbd_overview_dummy'])\n",
    "\n",
    "overview=pd.concat([walk_overview,bondi_overview,beach_overview,cbding_overview,walking_overview\n",
    "                    ,habour_overview,centre_overview,short_overview,city_overview],axis=1)\n",
    "des=pd.concat([walk,beach,views,city,large,apartment],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827fbd8",
   "metadata": {},
   "source": [
    "## room_tyoe, instant bookable, property type, minimum night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a84551",
   "metadata": {},
   "outputs": [],
   "source": [
    "## room_type\n",
    "room_type_dummy=np.select(\n",
    "    condlist=[ \n",
    "        train['room_type'].str.contains('Private', case=False),\n",
    "        train['room_type'].str.contains('entire', case=False),\n",
    "        train['room_type'].str.contains('shared', case=False),\n",
    "        ],\n",
    "    choicelist=['Private', 'entire', 'shared'],\n",
    "    default='Hotel'\n",
    ")\n",
    "room_type_dummy = pd.DataFrame(room_type_dummy,columns=['room_type_dummy'])\n",
    "room_type_dummy=pd.get_dummies(room_type_dummy)\n",
    "## Instant bookable\n",
    "instant_bookable_dummy=pd.get_dummies(train['instant_bookable'],drop_first = True)\n",
    "instant_bookable_dummy.columns=['instant_bookable_dummy']\n",
    "# property type\n",
    "# la=train['property_type']\n",
    "# property_type_dummy = np.select(\n",
    "#     condlist=[\n",
    "#         la.str.contains('Private', case=False),\n",
    "#         la.str.contains('entire', case=False),\n",
    "#         ],\n",
    "#     choicelist=['Private', 'entire'],\n",
    "#     default='others'\n",
    "# )\n",
    "\n",
    "property_type_dummy = pd.DataFrame(train['property_type'])\n",
    "property_type_dummy=pd.get_dummies(property_type_dummy,drop_first = True)\n",
    "# Minimum night\n",
    "minimum_night=train['minimum_nights']\n",
    "\n",
    "# minimum_night.values[minimum_night < 28] = 0\n",
    "# minimum_night.values[minimum_night >= 28] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a27a6",
   "metadata": {},
   "source": [
    "## house_response_time, house_response_rate, house_acceptance_rate, host_is_superhost, host_identity_verified, host_verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a19f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"host_response_time\"] = train[\"host_response_time\"].fillna('NA')\n",
    "host_response_time_dummy = np.select(\n",
    "    condlist=[ \n",
    "       train['host_response_time'].str.contains('within an hour', case=False),\n",
    "        train['host_response_time'].str.contains('within a few hours', case=False),\n",
    "        train['host_response_time'].str.contains('within a day', case=False),\n",
    "        train['host_response_time'].str.contains('a few days or more', case=False),\n",
    "        ],\n",
    "    choicelist=['within an hour', 'within a few hours', 'within a day','a few days or more'],\n",
    "    default='others'\n",
    ")\n",
    "host_response_time_dummy = pd.DataFrame(host_response_time_dummy,columns=['host_response_time_dummy'])\n",
    "host_response_time_dummy = pd.get_dummies(host_response_time_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332216a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"host_response_rate\"] = train[\"host_response_rate\"].astype('str')\n",
    "train[\"host_response_rate\"] = train[\"host_response_rate\"].str.replace(\"%\",\"\")\n",
    "train[\"host_response_rate\"] = train[\"host_response_rate\"].astype('float')\n",
    "def hostrrate(rrate):\n",
    "    if rrate > 75:\n",
    "        return 'rrate over 75'\n",
    "    elif rrate > 50:\n",
    "        return 'rrate over 50'\n",
    "    else:\n",
    "        return 'rrate under 50'\n",
    "train[\"host_response_rate\"] = train[\"host_response_rate\"].apply(hostrrate)\n",
    "host_response_rate_dummy = np.select(\n",
    "    condlist=[ \n",
    "       train['host_response_rate'].str.contains('rrate over 50', case=False),\n",
    "        train['host_response_rate'].str.contains('rrate over 75', case=False),\n",
    "        ],\n",
    "    choicelist=['rrate over 50', 'rrate over 75',],\n",
    "    default='rrate under 50'\n",
    ")\n",
    "host_response_rate_dummy = pd.DataFrame(host_response_rate_dummy,columns=['host_response_rate_dummy'])\n",
    "host_response_rate_dummy = pd.get_dummies(host_response_rate_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"host_acceptance_rate\"] = train[\"host_acceptance_rate\"].astype('str')\n",
    "train[\"host_acceptance_rate\"] = train[\"host_acceptance_rate\"].str.replace(\"%\",\"\")\n",
    "train[\"host_acceptance_rate\"] = train[\"host_acceptance_rate\"].astype('float')\n",
    "def hostarate(arate):\n",
    "    if arate > 75:\n",
    "        return 'arate over 75'\n",
    "    elif arate > 50:\n",
    "        return 'arate over 50'\n",
    "    else:\n",
    "        return 'arate under 50'\n",
    "train[\"host_acceptance_rate\"] = train[\"host_acceptance_rate\"].apply(hostarate)\n",
    "host_acceptance_rate_dummy = np.select(\n",
    "    condlist=[ \n",
    "       train['host_acceptance_rate'].str.contains('rrate over 50', case=False),\n",
    "        train['host_acceptance_rate'].str.contains('rrate over 75', case=False),\n",
    "        ],\n",
    "    choicelist=['rrate over 50', 'rrate over 75',],\n",
    "    default='rrate under 50'\n",
    ")\n",
    "host_acceptance_rate_dummy = pd.DataFrame(host_acceptance_rate_dummy,columns=['host_acceptance_rate_dummy'])\n",
    "host_acceptance_rate_dummy = pd.get_dummies(host_acceptance_rate_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_is_superhost_dummy=pd.get_dummies(train['host_is_superhost'],drop_first = True)\n",
    "host_is_superhost_dummy.columns=['host_is_superhost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_identity_verified_dummy=pd.get_dummies(train['host_identity_verified'],drop_first = True)\n",
    "host_identity_verified_dummy.columns=['host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_identity_verified_dummy=pd.get_dummies(train['host_identity_verified'],drop_first = True)\n",
    "host_identity_verified_dummy.columns=['host_identity_verified']\n",
    "\n",
    "train.loc[train[\"host_verifications\"] == \"['email', 'phone', 'work_email']\", \"host_verifications\"] = 'Three types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['email', 'phone']\", \"host_verifications\"] = 'Two types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['phone', 'work_email']\", \"host_verifications\"] = 'Two types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['email']\", \"host_verifications\"] = 'One types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['phone']\", \"host_verifications\"] = 'One types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"[]\", \"host_verifications\"] = 'Two types of verifications'\n",
    "host_verifications_dummy = np.select(\n",
    "    condlist=[ \n",
    "       train['host_verifications'].str.contains('One types of verifications', case=False),\n",
    "        train['host_verifications'].str.contains('Two types of verifications', case=False),\n",
    "        ],\n",
    "    choicelist=['One types of verifications', 'Two types of verifications',],\n",
    "    default='Three types of verifications'\n",
    ")\n",
    "host_verifications_dummy = pd.DataFrame(host_verifications_dummy,columns=['host_verifications_dummy'])\n",
    "host_verifications_dummy = pd.get_dummies(host_verifications_dummy,drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_identity_verified_dummy=pd.get_dummies(train['host_identity_verified'],drop_first = True)\n",
    "host_identity_verified_dummy.columns=['host_identity_verified']\n",
    "\n",
    "train.loc[train[\"host_verifications\"] == \"['email', 'phone', 'work_email']\", \"host_verifications\"] = 'Three types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['email', 'phone']\", \"host_verifications\"] = 'Two types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['email', 'work_email']\", \"host_verifications\"] = 'Two types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['phone', 'work_email']\", \"host_verifications\"] = 'Two types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['email']\", \"host_verifications\"] = 'One types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['phone']\", \"host_verifications\"] = 'One types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"['work_email']\", \"host_verifications\"] = 'One types of verifications'\n",
    "train.loc[train[\"host_verifications\"] == \"[]\", \"host_verifications\"] = 'Zero types of verifications'\n",
    "host_verifications_dummy = np.select(\n",
    "    condlist=[ \n",
    "       train['host_verifications'].str.contains('One types of verifications', case=False),\n",
    "        train['host_verifications'].str.contains('Two types of verifications', case=False),\n",
    "        train['host_verifications'].str.contains('Three types of verifications', case=False),\n",
    "        ],\n",
    "    choicelist=['One types of verifications', 'Two types of verifications','Three types of verifications'],\n",
    "    default='Zero types of verifications'\n",
    ")\n",
    "host_verifications_dummy = pd.DataFrame(host_verifications_dummy,columns=['host_verifications_dummy'])\n",
    "host_verifications_dummy = pd.get_dummies(host_verifications_dummy,drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['log_number_of_reviews'] = np.log(train['number_of_reviews'])\n",
    "\n",
    "log_number_of_reviews=train['log_number_of_reviews']\n",
    "\n",
    "\n",
    "review_scores_rating=train['review_scores_rating']\n",
    "\n",
    "review_scores_rating.values[review_scores_rating < 4.5] = 0\n",
    "review_scores_rating.values[review_scores_rating >= 4.5] = 1\n",
    "\n",
    "review_scores_accuracy=train['review_scores_accuracy']\n",
    "\n",
    "review_scores_accuracy.values[review_scores_accuracy < 4.5] = 0\n",
    "review_scores_accuracy.values[review_scores_accuracy >= 4.5] = 1\n",
    "\n",
    "review_scores_cleanliness=train['review_scores_cleanliness']\n",
    "\n",
    "review_scores_cleanliness.values[review_scores_cleanliness < 4.5] = 0\n",
    "review_scores_cleanliness.values[review_scores_cleanliness >= 4.5] = 1\n",
    "\n",
    "review_scores_checkin=train['review_scores_checkin']\n",
    "\n",
    "review_scores_checkin.values[review_scores_checkin < 4.5] = 0\n",
    "review_scores_checkin.values[review_scores_checkin >= 4.5] = 1\n",
    "\n",
    "review_scores_communication=train['review_scores_communication']\n",
    "\n",
    "review_scores_communication.values[review_scores_communication < 4.5] = 0\n",
    "review_scores_communication.values[review_scores_communication >= 4.5] = 1\n",
    "\n",
    "review_scores_location=train['review_scores_location']\n",
    "\n",
    "review_scores_location.values[review_scores_location < 4.5] = 0\n",
    "review_scores_location.values[review_scores_location >= 4.5] = 1\n",
    "\n",
    "review_scores_value=train['review_scores_value']\n",
    "\n",
    "review_scores_value.values[review_scores_value < 4.5] = 0\n",
    "review_scores_value.values[review_scores_value >= 4.5] = 1\n",
    "\n",
    "reviews_per_month=train['reviews_per_month']\n",
    "\n",
    "reviews_per_month.values[reviews_per_month < 4.5] = 0\n",
    "reviews_per_month.values[reviews_per_month >= 4.5] = 1\n",
    "\n",
    "\n",
    "\n",
    "# shenwei=pd.concat([log_number_of_reviews,review_scores_rating,review_scores_accuracy,review_scores_cleanliness, review_scores_checkin,review_scores_communication,review_scores_value,reviews_per_month].axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972b3d9",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b21bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location = pd.get_dummies(train['neighbourhood_cleansed'])\n",
    "Location = train[['latitude','longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "emily = pd.concat([host_response_time_dummy, host_response_rate_dummy,host_acceptance_rate_dummy,host_is_superhost_dummy,train['host_listings_count'],host_verifications_dummy],axis=1)\n",
    "# emily = pd.concat([host_response_time_dummy, host_response_rate_dummy,host_acceptance_rate_dummy,host_is_superhost_dummy,host_verifications_dummy,train['host_listings_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f780b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['start_year'] = pd.to_datetime(train['host_since']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['bedrooms'],train['beds'],\n",
    "train['neighborhood_overview'] = train['neighborhood_overview'].fillna(train['neighbourhood'])\n",
    "neighbourhood_cleansed_dummy=pd.get_dummies(train['neighbourhood_cleansed'],drop_first = True)\n",
    "\n",
    "neighbourhooddummy=pd.get_dummies(train['neighbourhood'],drop_first = True)\n",
    "\n",
    "# train['log_number_of_reviews'] = np.log(train['number_of_reviews'])\n",
    "\n",
    "# log_number_of_reviews=train['log_number_of_reviews']\n",
    "\n",
    "# log_number_of_reviews.values[log_number_of_reviews < 4.5] = 0\n",
    "# log_number_of_reviews.values[log_number_of_reviews >= 4.5] = 1\n",
    "\n",
    "# review_scores_rating=train['review_scores_rating']\n",
    "\n",
    "# review_scores_rating.values[review_scores_rating < 4.5] = 0\n",
    "# review_scores_rating.values[review_scores_rating >= 4.5] = 1\n",
    "\n",
    "\n",
    "\n",
    "zeshen=pd.concat([des,overview,train['minimum_nights'],train['maximum_nights'],\n",
    "                  property_type_dummy,instant_bookable_dummy,train['accommodates']],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combine = pd.concat([zeshen,Location,bow_df,log_number_of_reviews, review_scores_rating,review_scores_accuracy, \n",
    "                     review_scores_cleanliness,review_scores_location ,review_scores_checkin,review_scores_communication,review_scores_value,reviews_per_month],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ffc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = combine.loc[:,~combine.columns.duplicated()]\n",
    "duplicate_columns = combine.columns[combine.columns.duplicated()]\n",
    "print(duplicate_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0aefe2",
   "metadata": {},
   "source": [
    "## Advanced Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combine[:4000], y, test_size = 0.2,random_state = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7f06d",
   "metadata": {},
   "source": [
    "## Tree: cost complexity pruned regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(min_samples_leaf=5, random_state=1)\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "model = DecisionTreeRegressor(min_samples_leaf=5, random_state=1)\n",
    "\n",
    "tuning_parameters = {'ccp_alpha': path.ccp_alphas,}\n",
    "tree_cv = GridSearchCV(model, tuning_parameters, cv=5, return_train_score=False)\n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "tree = tree_cv.best_estimator_\n",
    "tree_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = list(combine[:4000].columns.values[:])\n",
    "# len(predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5\n",
    "\n",
    "importance = 100*(tree.feature_importances_/np.max(tree.feature_importances_))\n",
    "feature_importance = pd.Series(importance, index=predictors).sort_values(ascending=True)\n",
    "plt.figure(figsize=(27, 10))\n",
    "plt.barh(np.arange(p), feature_importance[-p:])\n",
    "plt.yticks(np.arange(p), feature_importance[-p:].index, fontsize=17)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.xlabel('% of maximum importance', fontsize=20)\n",
    "plt.title('Figure 2. Variable importance of top {} predictors'.format(p), fontsize=20);\n",
    "# plt.savefig('top 5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65828dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = pd.DataFrame(columns=['RMSE', 'R2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b98628",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(tree, X_train, y_train, cv=5)\n",
    "\n",
    "rmse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "\n",
    "validation_results.loc['Regressor decision tree'] = rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(35, 15))\n",
    "plot_tree(tree, rounded=True, filled=True, feature_names=predictors, max_depth=2, fontsize=35)\n",
    "plt.title('Figure 1. Cost complexity pruned tree', fontsize=50);\n",
    "# plt.savefig('tree.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4aabcb",
   "metadata": {},
   "source": [
    "## Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag = BaggingRegressor(DecisionTreeRegressor(), n_estimators=200, random_state=1)\n",
    "bag.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c567eb",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8072e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=1)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': [10, 50, 100],\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(model, tuning_parameters, cv=5, return_train_score=False, n_jobs=4)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "rf = rf_cv.best_estimator_\n",
    "\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.n_estimators = 2000\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2ae3b",
   "metadata": {},
   "source": [
    "## Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701124a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=2000, subsample=0.5)\n",
    "gb.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b136d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba1895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = 5\n",
    "\n",
    "importance = 100*(gb.feature_importances_/np.max(gb.feature_importances_))\n",
    "feature_importance = pd.Series(importance, index=predictors).sort_values(ascending=True)\n",
    "plt.figure(figsize=(35, 15))\n",
    "plt.barh(np.arange(p), feature_importance[-p:])\n",
    "plt.yticks(np.arange(p), feature_importance[-p:].index)\n",
    "plt.xlabel('% of maximum importance')\n",
    "plt.title('Figure 2. Variable importance of top {} predictors'.format(p));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddeee50",
   "metadata": {},
   "source": [
    "## Addictive boosting\n",
    "boost a lasso regression with a random forest model, and a Gradient Boost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25ce6e",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc14e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ols = LinearRegression()\n",
    "# ols.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV(cv=5)\n",
    "lasso.fit(X_train_scaled, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = lasso.predict(X_train_scaled)\n",
    "resid = y_train - y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5abe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## regid \n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-5, 4, 151)\n",
    "\n",
    "ridge = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge.fit(X_train_scaled, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "\n",
    "elastic_net = ElasticNetCV(l1_ratio=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99], cv=5)\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "best_elastic_net = ElasticNet(l1_ratio=elastic_net.l1_ratio_, alpha=elastic_net.alpha_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf23b2",
   "metadata": {},
   "source": [
    "### a. random forest boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\"max_features\":  [100,150,200],\n",
    "#               \"n_estimators\": [1000,1500,2000],\n",
    "#               \"min_samples_leaf\": [1,2,3]}\n",
    "# search = RandomizedSearchCV(RandomForestRegressor(), param_grid, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# rf_boost = RandomForestRegressor(n_estimators=search.best_params_[\"n_estimators\"], min_samples_leaf=search.best_params_[\"min_samples_leaf\"],\n",
    "#                                  max_features=search.best_params_[\"max_features\"], random_state=1, n_jobs=4)\n",
    "rf_boost = RandomForestRegressor(n_estimators=1000, min_samples_leaf=1,\n",
    "                                 max_features=150, random_state=1, n_jobs=4)\n",
    "rf_boost.fit(X_train, resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1323b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('R2 Value:',metrics.r2_score(y_train, rf_boost.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(X_test_scaled) + rf_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72894b",
   "metadata": {},
   "source": [
    "### b. gradient boost boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\"max_depth\":    [2,3,4,5,6],\n",
    "#               \"n_estimators\": [700,850,1000,1500],\n",
    "#               \"learning_rate\": [0.01, 0.015,0.02]}\n",
    "# search = RandomizedSearchCV(GradientBoostingRegressor(), param_grid, cv=5).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# gb_boost = GradientBoostingRegressor(learning_rate=search.best_params_[\"n_estimators\"],\n",
    "#                                      max_depth=search.best_params_[\"max_depth\"],\n",
    "#                                      n_estimators=search.best_params_[\"learning_rate\"], subsample=0.5)\n",
    "\n",
    "gb_boost = GradientBoostingRegressor(learning_rate=0.015,\n",
    "                                     max_depth=2,\n",
    "                                     n_estimators=2000, subsample=0.5)\n",
    "gb_boost.fit(X_train, resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038217ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('R2 Value:',metrics.r2_score(y_train, gb_boost.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ea341",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(X_test_scaled) + gb_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ef2a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## adaboost boost\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "DTR=DecisionTreeRegressor(max_depth=2)\n",
    "ada = AdaBoostRegressor(random_state=1, base_estimator=DTR, n_estimators=300)\n",
    "ada.fit(X_train, y_train)\n",
    "print('R2 Value:',metrics.r2_score(y_train, ada.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd044fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# fit model on training data\n",
    "regressor=XGBRegressor(eval_metric='rmse')\n",
    "param_grid = {\"max_depth\":    [6],\n",
    "              \"n_estimators\": [800],\n",
    "              \"learning_rate\": [0.015],\n",
    "             'min_child_weight':[1]}\n",
    "# search = GridSearchCV(regressor, param_grid, cv=5).fit(X_train, y_train)\n",
    "xgb=XGBRegressor(learning_rate = search.best_params_[\"learning_rate\"],\n",
    "                           n_estimators  = search.best_params_[\"n_estimators\"],\n",
    "                           max_depth     = search.best_params_[\"max_depth\"],\n",
    "               sampling_method= 'gradient_based',subsample=0.1,tree_method='gpu_hist',\n",
    "                 objective='reg:squarederror',booster='gbtree',reg_lambda=0,reg_alpha=0, n_jobs=4,\n",
    "                 process_type='default')\n",
    "\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447def08",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbb=xgb.fit(X_train, resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf32dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(X_test_scaled) + xgbb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dde493",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Value:',metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best hyperparameters are \",search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Value:',metrics.r2_score(y_train, xgb.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56008751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Value:',metrics.r2_score(y_test, xgb.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d648cc4",
   "metadata": {},
   "source": [
    "## Model stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "models = [('Ridge Regression', ridge), ('Gradient Boost', gb), ('lasso',lasso),('Gradient BoostBoost', gb_boost),(\"Random forest boost\",rf_boost),('xgboost',xgb),('xgboost boost',xgbb)('ada',ada)]\n",
    "stack = StackingRegressor(models, final_estimator=LinearRegression(positive=True), cv=5, n_jobs=4)\n",
    "stack.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9319cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 Value:',metrics.r2_score(y_test, stack.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(np.arange(len(models)), stack.final_estimator_.coef_)\n",
    "plt.yticks(np.arange(len(models)), ['Ridge Regression','lasso', 'Gradient Boost', 'Gradient BoostBoost',\"Random forest boost\",'xgboost','ada']);\n",
    "plt.xlabel('Model coefficient')\n",
    "plt.title('Figure 4. Model coefficients for our stacked model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89402f4d",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820713e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "#     'OLS': ols,\n",
    "    'Lasso': lasso,\n",
    "    'Ridge': ridge,\n",
    "    'Elastic Net': elastic_net,\n",
    "    'Tree': tree,\n",
    "    'Bgged': bag,\n",
    "    'Random Forest': rf,\n",
    "    'Gradient Boost': gb,\n",
    "    'Model Stack': stack,\n",
    "    'Random Forest Boost': rf_boost,\n",
    "    'Gradient Boost Boost': gb_boost,\n",
    "    'Ada':ada,\n",
    "    'XGboost':xgb,\n",
    "    'XGboost boost':xgbb\n",
    "}\n",
    "\n",
    "test = pd.DataFrame(columns=['RMSE', 'R2'])\n",
    "\n",
    "for name, model in methods.items():\n",
    "    if name in ['Lasso', 'Ridge', 'Elastic Net']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    elif name in ['Random Forest Boost', 'Gradient Boost Boost',\"'XGboost boost'\"]:\n",
    "        y_pred = lasso.predict(X_test_scaled) + model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    test.loc[name] = rmse, r2\n",
    "\n",
    "test.round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa298ce",
   "metadata": {},
   "source": [
    "## Prediction: Model stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0408e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = combine[:4000]\n",
    "X_test = combine[4000:6000]\n",
    "y_train = y\n",
    "\n",
    "models = [('Ridge Regression', ridge), ('Gradient Boost', gb), ('Random forest boost',rf_boost),('Gradient BoostBoost', gb_boost),(\"xgboost\",xgb)]\n",
    "\n",
    "stack = StackingRegressor(models, final_estimator=LassoCV(positive=True,cv=5), cv=5, n_jobs=4)\n",
    "# stack = StackingRegressor(models, final_estimator=LinearRegression(positive=True), cv=5, n_jobs=4)\n",
    "stack.fit(X_train, y_train);\n",
    "\n",
    "y_pred = stack.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "y_fit_ms = stack.predict(X_train)\n",
    "resid_ms = y_train - y_fit_ms\n",
    "\n",
    "prediction = np.exp(y_pred)*np.mean(np.exp(resid_ms))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_fit_gb = lasso.predict(X_train_scaled) + gb_boost.predict(X_train)\n",
    "# resid_gb = y_train - y_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = np.exp(y_pred)*np.mean(np.exp(resid_gb))\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(prediction,columns=['price'])\n",
    "submission.index.names = ['id']\n",
    "submission=submission['price']\n",
    "submission = pd.DataFrame(submission).to_csv('Gradiant boost_submission_zeshen.csv')\n",
    "\n",
    "# submission = pd.read_csv('Gradiant boost_submission.csv').drop('Unnamed: 0',axis=1)\n",
    "# submission\n",
    "# print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81fdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
